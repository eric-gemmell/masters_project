{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90cc068c",
   "metadata": {},
   "source": [
    "# Only run this if you need to tbh, you dont want rewrite the json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371bdc48",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "# standard library modules\n",
    "import sys, errno, re, json, ssl\n",
    "from urllib import request\n",
    "from urllib.error import HTTPError\n",
    "from time import sleep\n",
    "from Bio.SeqRecord import SeqRecord\n",
    "from Bio.Seq import Seq\n",
    "import requests\n",
    "import xmltodict\n",
    "import concurrent.futures\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "172b16e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../processed_sequences/annotated_sequences.json', 'w') as f:\n",
    "    json.dump([],f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f33f4eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_additional_info(seq_data):\n",
    "    accession = seq_data[\"Accession_Interpro\"]\n",
    "    url = f\"https://rest.uniprot.org/uniprotkb/{accession}.xml\"\n",
    "    response = requests.get(url)\n",
    "    xml_data = xmltodict.parse(response.text)\n",
    "    if(response.status_code == 200):\n",
    "        seq_data[\"lineage\"] = xml_data['uniprot']['entry']['organism']['lineage'][\"taxon\"]\n",
    "        seq_data[\"Accession_RefSeq\"] = \"\"\n",
    "        seq_data[\"Accession_AlphaFoldDB\"] = \"\"\n",
    "        for reference in xml_data['uniprot']['entry']['dbReference']:\n",
    "            try:\n",
    "                if reference['@type'] == 'RefSeq' or reference['@type'] == 'AlphaFoldDB':\n",
    "                    id = reference['@id']\n",
    "                    seq_data[\"Accession_\"+reference[\"@type\"]] = id\n",
    "            except:\n",
    "                print(f\"\\n Failed to get info for {accession}\")\n",
    "                \n",
    "    else:\n",
    "        print(f\"FAILED RETRIEVING ADDITIONAL DATA FOR {accession}\")\n",
    "    return seq_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13525ac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "raise Exception(\"You don't want to run me unless you know what you're doing\")\n",
    "\n",
    "BASE_URL = \"https://www.ebi.ac.uk:443/interpro/api/protein/UniProt/entry/InterPro/IPR003776/?page_size=200&extra_fields=sequence&taxonomy\"\n",
    "\n",
    "HEADER_SEPARATOR = \"|\"\n",
    "\n",
    "\n",
    "def output_list(next = BASE_URL):\n",
    "  #disable SSL verification to avoid config issues\n",
    "  context = ssl._create_unverified_context()\n",
    "  last_page = False\n",
    "  total_data = []\n",
    "\n",
    "  if(next != BASE_URL):\n",
    "        print(\"Not starting from the beginning! Loading previous Json!\")\n",
    "        with open('../processed_sequences/annotated_sequences.json', 'r') as f:\n",
    "            # load the data from the file\n",
    "            total_data = json.load(f)\n",
    "        print(f\"We have a total of {len(total_data)} sequences!\")\n",
    "        print(\"\\nContinuing where we left off!...\\n\")\n",
    "        \n",
    "  attempts = 0\n",
    "  while next:\n",
    "    try:\n",
    "      req = request.Request(next, headers={\"Accept\": \"application/json\"})\n",
    "      res = request.urlopen(req, context=context)\n",
    "      # If the API times out due a long running query\n",
    "      if res.status == 408:\n",
    "        # wait just over a minute\n",
    "        sleep(61)\n",
    "        # then continue this loop with the same URL\n",
    "        continue\n",
    "      elif res.status == 204:\n",
    "        #no data so leave loop\n",
    "        break\n",
    "      payload = json.loads(res.read().decode())\n",
    "      next = payload[\"next\"]\n",
    "      print(next)\n",
    "      \n",
    "      attempts = 0\n",
    "      if not next:\n",
    "        last_page = True\n",
    "    except HTTPError as e:\n",
    "      if e.code == 408:\n",
    "        sleep(61)\n",
    "        continue\n",
    "      else:\n",
    "        # If there is a different HTTP error, it wil re-try 3 times before failing\n",
    "        if attempts < 3:\n",
    "          attempts += 1\n",
    "          sleep(61)\n",
    "          continue\n",
    "        else:\n",
    "          sys.stderr.write(\"LAST URL: \" + next)\n",
    "          raise e\n",
    "    data = []\n",
    "    for i, item in enumerate(payload[\"results\"]):\n",
    "      entries = None\n",
    "      if (\"entry_subset\" in item):\n",
    "        entries = item[\"entry_subset\"]\n",
    "      elif (\"entries\" in item):\n",
    "        entries = item[\"entries\"]\n",
    "      \n",
    "      seq_data = {}\n",
    "      if entries is not None:\n",
    "        start = 0\n",
    "        end = 0\n",
    "        for entry in entries:\n",
    "          for locations in entry['entry_protein_locations']:\n",
    "            for fragment in locations['fragments']:\n",
    "              start = fragment['start']\n",
    "              end = fragment['end']\n",
    "        \n",
    "        seq_data[\"Accession_Interpro\"] = item[\"metadata\"][\"accession\"]\n",
    "        seq_data[\"YcaO_domain\"] = {\"start\":start, \"end\":end}\n",
    "      seq_data[\"seq\"] = item[\"extra_fields\"][\"sequence\"]\n",
    "      data.append(seq_data)\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=8) as executor:\n",
    "        # submit a task to retrieve information for each accession\n",
    "        tasks = [executor.submit(get_additional_info, seq_data) for seq_data in data]\n",
    "\n",
    "        # retrieve the results of the tasks as they complete\n",
    "        results = [task.result() for task in concurrent.futures.as_completed(tasks)]\n",
    "\n",
    "    \n",
    "    total_data.extend(data)\n",
    "    print(f\"Processed {len(data)} in the last batch, total {len(total_data)} out of 17 000 sequences\")\n",
    "    # Don't overload the server, give it time before asking for more\n",
    "    with open('../processed_sequences/annotated_sequences.json', 'w') as f:\n",
    "        json.dump(total_data,f)\n",
    "        print(\"...Progress Saved!\")\n",
    "    with open(filename, \"a\") as f:\n",
    "        f.write(\"\\n\"+next)\n",
    "        \n",
    "  return total_data\n",
    "print(\"Hello\")\n",
    "filename = \"../processed_sequences/url_progress.txt\"\n",
    "total_data = []\n",
    "url = BASE_URL\n",
    "if os.path.exists(filename):\n",
    "    print(\"Identified pre existing save, loading... \")\n",
    "    with open(filename, \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "        if lines:\n",
    "            url = lines[-1]\n",
    "else:\n",
    "    print(\"Starting to load sequences and identifiers from scratch!\")\n",
    "    open(filename, \"w\").close()\n",
    "    \n",
    "total_data = output_list(url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af229188",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "total_data = []\n",
    "\n",
    "with open('../processed_sequences/annotated_sequences.json', 'r') as f:\n",
    "    total_data = json.load(f)\n",
    "#total_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d6be1b7",
   "metadata": {},
   "source": [
    "# Since the sequences have already been loaded, no need to run the previous script, just use the stuff below to reload the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a7a442f",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_data = []\n",
    "\n",
    "with open('../raw_sequences/annotated_sequences.json', 'r') as f:\n",
    "    total_data = json.load(f)\n",
    "#total_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f74d64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_list(data, result={}):\n",
    "    for item in data:\n",
    "        lineage = item[\"lineage\"]\n",
    "        if lineage:\n",
    "            lineage_dict = result\n",
    "            for level in lineage:\n",
    "                if level not in lineage_dict:\n",
    "                    lineage_dict[level] = {}\n",
    "                lineage_dict = lineage_dict[level]\n",
    "            lineage_dict.update(item)\n",
    "    return result\n",
    "\n",
    "transformed_data = transform_list(total_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3df89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sequences(data, family):\n",
    "    return [s for s in data if family in s['lineage']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4039d925",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def Draw_Pie(data, depth = 2):\n",
    "    # Create a defaultdict to store the count of each lineage\n",
    "    lineage_count = defaultdict(int)\n",
    "\n",
    "    # Iterate through the list of dictionaries and count the occurrences of each lineage\n",
    "    for item in data:\n",
    "        try:\n",
    "            lineage_count[item[\"lineage\"][depth]] += 1\n",
    "        except:\n",
    "            continue\n",
    "            \n",
    "    # Extract the labels (lineage names) and sizes (counts) for the pie chart\n",
    "    labels = list(lineage_count.keys())\n",
    "    sizes = list(lineage_count.values())\n",
    "\n",
    "    # Create the pie chart\n",
    "    plt.pie(sizes, labels=labels, autopct='%1.1f%%')\n",
    "    plt.axis('equal')  # Ensure the chart is a circle, not an ellipse\n",
    "    plt.legend(bbox_to_anchor=(1.1, 1.05))\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3803fd7f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Draw_Pie(total_data, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875cc9c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import Bio as bio\n",
    "from Bio import SeqIO\n",
    "from Bio.SeqRecord import SeqRecord\n",
    "from Bio.Seq import Seq\n",
    "import os\n",
    "\n",
    "family = \"Cyanobacteria\"\n",
    "filterByDomain = True\n",
    "\n",
    "sequences = get_sequences(total_data,family)\n",
    "seq_recs = []\n",
    "for seq_data in sequences:\n",
    "    seq = Seq(seq_data[\"seq\"])\n",
    "    if(filterByDomain):\n",
    "        seq = seq[seq_data[\"YcaO_domain\"][\"start\"]:seq_data[\"YcaO_domain\"][\"end\"]]\n",
    "    seq_recs.append(SeqRecord(seq, id=seq_data[\"Accession_Interpro\"]))\n",
    "\n",
    "dirname = f'../processed_sequences/{family}_sequences'\n",
    "filename = f\"{family}_{'YcaO_only' if filterByDomain else 'whole_protein'}.fa\"\n",
    "\n",
    "if not os.path.exists(dirname):\n",
    "    # Create the directory\n",
    "    os.makedirs(dirname)\n",
    "    \n",
    "SeqIO.write(seq_recs,os.path.join(dirname,filename), \"fasta\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb27fb0",
   "metadata": {},
   "source": [
    "# Loading the tridomain YcaOs, basically using the interpro domain definition rather than anything fancy I define myself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0193f9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To restart the download run this cell!\n",
    "!rm ../processed_sequences/tridomain_url_progress.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc61d8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tridomain_sequences_filename = '../processed_sequences/tridomain_annotated_sequences.json'\n",
    "with open(tridomain_sequences_filename, 'w') as f:\n",
    "    json.dump([],f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2973c847",
   "metadata": {},
   "outputs": [],
   "source": [
    "raise Exception(\"You have already run this code, sure you wanna do it again?\")\n",
    "\n",
    "BASE_URL = \"https://www.ebi.ac.uk:443/interpro/api/protein/UniProt/entry/InterPro/IPR022291/?page_size=200&extra_fields=sequence&taxonomy\"\n",
    "\n",
    "HEADER_SEPARATOR = \"|\"\n",
    "\n",
    "\n",
    "def output_list(next = BASE_URL):\n",
    "  #disable SSL verification to avoid config issues\n",
    "  context = ssl._create_unverified_context()\n",
    "  last_page = False\n",
    "  total_data = []\n",
    "\n",
    "  if(next != BASE_URL):\n",
    "        print(\"Not starting from the beginning! Loading previous Json!\")\n",
    "        with open(tridomain_sequences_filename, 'r') as f:\n",
    "            # load the data from the file\n",
    "            total_data = json.load(f)\n",
    "        print(f\"We have a total of {len(total_data)} sequences!\")\n",
    "        print(\"\\nContinuing where we left off!...\\n\")\n",
    "        \n",
    "  attempts = 0\n",
    "  while next:\n",
    "    try:\n",
    "      req = request.Request(next, headers={\"Accept\": \"application/json\"})\n",
    "      res = request.urlopen(req, context=context)\n",
    "      # If the API times out due a long running query\n",
    "      if res.status == 408:\n",
    "        # wait just over a minute\n",
    "        sleep(61)\n",
    "        # then continue this loop with the same URL\n",
    "        continue\n",
    "      elif res.status == 204:\n",
    "        #no data so leave loop\n",
    "        break\n",
    "      payload = json.loads(res.read().decode())\n",
    "      next = payload[\"next\"]\n",
    "      \n",
    "      attempts = 0\n",
    "      if not next:\n",
    "        last_page = True\n",
    "    except HTTPError as e:\n",
    "      if e.code == 408:\n",
    "        sleep(61)\n",
    "        continue\n",
    "      else:\n",
    "        # If there is a different HTTP error, it wil re-try 3 times before failing\n",
    "        if attempts < 3:\n",
    "          attempts += 1\n",
    "          sleep(61)\n",
    "          continue\n",
    "        else:\n",
    "          sys.stderr.write(\"LAST URL: \" + next)\n",
    "          raise e\n",
    "    data = []\n",
    "    for i, item in enumerate(payload[\"results\"]):\n",
    "      entries = None\n",
    "      if (\"entry_subset\" in item):\n",
    "        entries = item[\"entry_subset\"]\n",
    "      elif (\"entries\" in item):\n",
    "        entries = item[\"entries\"]\n",
    "      seq_data = {}\n",
    "      if entries is not None:\n",
    "        start = 0\n",
    "        end = 0\n",
    "        for entry in entries:\n",
    "          for locations in entry['entry_protein_locations']:\n",
    "            for fragment in locations['fragments']:\n",
    "              start = fragment['start']\n",
    "              end = fragment['end']\n",
    "        \n",
    "        seq_data[\"Accession_Interpro\"] = item[\"metadata\"][\"accession\"]\n",
    "        seq_data[\"Dehydratase_domain\"] = {\"start\":start, \"end\":end}\n",
    "      seq_data[\"seq\"] = item[\"extra_fields\"][\"sequence\"]\n",
    "      data.append(seq_data)\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=8) as executor:\n",
    "        # submit a task to retrieve information for each accession\n",
    "        tasks = [executor.submit(get_additional_info, seq_data) for seq_data in data]\n",
    "\n",
    "        # retrieve the results of the tasks as they complete\n",
    "        results = [task.result() for task in concurrent.futures.as_completed(tasks)]\n",
    "\n",
    "    \n",
    "    total_data.extend(data)\n",
    "    print(f\"Processed {len(data)} in the last batch, total {len(total_data)} out of 5 000 sequences\")\n",
    "    # Don't overload the server, give it time before asking for more\n",
    "    with open(tridomain_sequences_filename, 'w') as f:\n",
    "        json.dump(total_data,f)\n",
    "        print(\"...Progress Saved!\")\n",
    "    \n",
    "    with open(filename, \"a\") as f:\n",
    "        f.write(\"\\n\"+next)\n",
    "        \n",
    "  return total_data\n",
    "\n",
    "filename = \"../processed_sequences/tridomain_url_progress.txt\"\n",
    "total_data = []\n",
    "url = BASE_URL\n",
    "if os.path.exists(filename):\n",
    "    print(\"Identified pre existing save, loading... \")\n",
    "    with open(filename, \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "        if lines:\n",
    "            url = lines[-1]\n",
    "else:\n",
    "    print(\"Starting to load sequences and identifiers from scratch!\")\n",
    "    open(filename, \"w\").close()\n",
    "    \n",
    "total_data = output_list(url)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b5c13d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "YcaO_data = []\n",
    "with open('../raw_sequences/annotated_sequences.json', 'r') as f:\n",
    "    YcaO_data = json.load(f)\n",
    "    \n",
    "    \n",
    "E1_data = []\n",
    "\n",
    "with open('../raw_sequences/cyclodehydratase_annotaded_sequences.json', 'r') as f:\n",
    "    E1_data = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a84f61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "E1_subset = []\n",
    "E1_containing_accessions = [e1[\"Accession_Interpro\"] for e1 in E1_data]\n",
    "for ycao in YcaO_data:\n",
    "    if ycao[\"Accession_Interpro\"] in E1_containing_accessions:\n",
    "        E1_subset.append(ycao)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7150c2db",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(E1_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "395147c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
