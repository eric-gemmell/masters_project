{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ea85fb79",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "# standard library modules\n",
    "import sys, errno, re, json, ssl\n",
    "from urllib import request\n",
    "from urllib.error import HTTPError\n",
    "from time import sleep\n",
    "from Bio.SeqRecord import SeqRecord\n",
    "from Bio.Seq import Seq\n",
    "import requests\n",
    "import xmltodict\n",
    "import concurrent.futures\n",
    "import json\n",
    "import os\n",
    "with open('../processed_sequences/annotated_sequences.json', 'w') as f:\n",
    "    json.dump([],f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "834141ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_additional_info(seq_data):\n",
    "    accession = seq_data[\"Accession_Interpro\"]\n",
    "    url = f\"https://rest.uniprot.org/uniprotkb/{accession}.xml\"\n",
    "    response = requests.get(url)\n",
    "    xml_data = xmltodict.parse(response.text)\n",
    "    if(response.status_code == 200):\n",
    "        seq_data[\"lineage\"] = xml_data['uniprot']['entry']['organism']['lineage'][\"taxon\"]\n",
    "        for reference in xml_data['uniprot']['entry']['dbReference']:\n",
    "            if reference['@type'] == 'RefSeq' or reference['@type'] == 'AlphaFoldDB':\n",
    "                id = reference['@id']\n",
    "                seq_data[\"Accession_\"+reference[\"@type\"]] = id\n",
    "    else:\n",
    "        print(f\"FAILED RETRIEVING ADDITIONAL DATA FOR {accession}\")\n",
    "    return seq_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "827950ce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n",
      "Identified pre existing save, loading... \n",
      "Not starting from the beginning! Loading previous Json!\n",
      "We have a total of 1600 sequences!\n",
      "\n",
      "Continuing where we left off!...\n",
      "\n",
      "https://www.ebi.ac.uk/interpro/api/protein/UniProt/entry/InterPro/IPR003776/?cursor=source%3As%3Aa0a1c0sxm0&extra_fields=sequence&page_size=200&taxonomy=\n",
      "Processed 200 in the last batch, total 1800 out of 17 000 sequences\n",
      "...Progress Saved!\n",
      "https://www.ebi.ac.uk/interpro/api/protein/UniProt/entry/InterPro/IPR003776/?cursor=source%3As%3Aa0a1e4dvk1&extra_fields=sequence&page_size=200&taxonomy=\n",
      "Processed 200 in the last batch, total 2000 out of 17 000 sequences\n",
      "...Progress Saved!\n",
      "https://www.ebi.ac.uk/interpro/api/protein/UniProt/entry/InterPro/IPR003776/?cursor=source%3As%3Aa0a1g3h2x4&extra_fields=sequence&page_size=200&taxonomy=\n",
      "Processed 200 in the last batch, total 2200 out of 17 000 sequences\n",
      "...Progress Saved!\n",
      "https://www.ebi.ac.uk/interpro/api/protein/UniProt/entry/InterPro/IPR003776/?cursor=source%3As%3Aa0a1h2cf51&extra_fields=sequence&page_size=200&taxonomy=\n",
      "Processed 200 in the last batch, total 2400 out of 17 000 sequences\n",
      "...Progress Saved!\n",
      "https://www.ebi.ac.uk/interpro/api/protein/UniProt/entry/InterPro/IPR003776/?cursor=source%3As%3Aa0a1h9za04&extra_fields=sequence&page_size=200&taxonomy=\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "BASE_URL = \"https://www.ebi.ac.uk:443/interpro/api/protein/UniProt/entry/InterPro/IPR003776/?page_size=200&extra_fields=sequence&taxonomy\"\n",
    "\n",
    "HEADER_SEPARATOR = \"|\"\n",
    "\n",
    "\n",
    "def output_list(next = BASE_URL):\n",
    "  #disable SSL verification to avoid config issues\n",
    "  context = ssl._create_unverified_context()\n",
    "  last_page = False\n",
    "  total_data = []\n",
    "\n",
    "  if(next != BASE_URL):\n",
    "        print(\"Not starting from the beginning! Loading previous Json!\")\n",
    "        with open('../processed_sequences/annotated_sequences.json', 'r') as f:\n",
    "            # load the data from the file\n",
    "            total_data = json.load(f)\n",
    "        print(f\"We have a total of {len(total_data)} sequences!\")\n",
    "        print(\"\\nContinuing where we left off!...\\n\")\n",
    "        \n",
    "  attempts = 0\n",
    "  while next:\n",
    "    try:\n",
    "      req = request.Request(next, headers={\"Accept\": \"application/json\"})\n",
    "      res = request.urlopen(req, context=context)\n",
    "      # If the API times out due a long running query\n",
    "      if res.status == 408:\n",
    "        # wait just over a minute\n",
    "        sleep(61)\n",
    "        # then continue this loop with the same URL\n",
    "        continue\n",
    "      elif res.status == 204:\n",
    "        #no data so leave loop\n",
    "        break\n",
    "      payload = json.loads(res.read().decode())\n",
    "      next = payload[\"next\"]\n",
    "      print(next)\n",
    "      \n",
    "      attempts = 0\n",
    "      if not next:\n",
    "        last_page = True\n",
    "    except HTTPError as e:\n",
    "      if e.code == 408:\n",
    "        sleep(61)\n",
    "        continue\n",
    "      else:\n",
    "        # If there is a different HTTP error, it wil re-try 3 times before failing\n",
    "        if attempts < 3:\n",
    "          attempts += 1\n",
    "          sleep(61)\n",
    "          continue\n",
    "        else:\n",
    "          sys.stderr.write(\"LAST URL: \" + next)\n",
    "          raise e\n",
    "    data = []\n",
    "    for i, item in enumerate(payload[\"results\"]):\n",
    "      entries = None\n",
    "      if (\"entry_subset\" in item):\n",
    "        entries = item[\"entry_subset\"]\n",
    "      elif (\"entries\" in item):\n",
    "        entries = item[\"entries\"]\n",
    "      \n",
    "      seq_data = {}\n",
    "      if entries is not None:\n",
    "        start = 0\n",
    "        end = 0\n",
    "        for entry in entries:\n",
    "          for locations in entry['entry_protein_locations']:\n",
    "            for fragment in locations['fragments']:\n",
    "              start = fragment['start']\n",
    "              end = fragment['end']\n",
    "        \n",
    "        seq_data[\"Accession_Interpro\"] = item[\"metadata\"][\"accession\"]\n",
    "        seq_data[\"YcaO_domain\"] = {\"start\":start, \"end\":end}\n",
    "      seq_data[\"seq\"] = item[\"extra_fields\"][\"sequence\"]\n",
    "      data.append(seq_data)\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=8) as executor:\n",
    "        # submit a task to retrieve information for each accession\n",
    "        tasks = [executor.submit(get_additional_info, seq_data) for seq_data in data]\n",
    "\n",
    "        # retrieve the results of the tasks as they complete\n",
    "        results = [task.result() for task in concurrent.futures.as_completed(tasks)]\n",
    "\n",
    "    \n",
    "    total_data.extend(data)\n",
    "    print(f\"Processed {len(data)} in the last batch, total {len(total_data)} out of 17 000 sequences\")\n",
    "    # Don't overload the server, give it time before asking for more\n",
    "    with open('../processed_sequences/annotated_sequences.json', 'w') as f:\n",
    "        json.dump(total_data,f)\n",
    "        print(\"...Progress Saved!\")\n",
    "    with open(filename, \"a\") as f:\n",
    "        f.write(\"\\n\"+next)\n",
    "        \n",
    "  return total_data\n",
    "print(\"Hello\")\n",
    "filename = \"../processed_sequences/url_progress.txt\"\n",
    "total_data = []\n",
    "url = BASE_URL\n",
    "if os.path.exists(filename):\n",
    "    print(\"Identified pre existing save, loading... \")\n",
    "    with open(filename, \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "        if lines:\n",
    "            url = lines[-1]\n",
    "else:\n",
    "    print(\"Starting to load sequences and identifiers from scratch!\")\n",
    "    open(filename, \"w\").close()\n",
    "    \n",
    "total_data = output_list(url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d94d67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f43aef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
